{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b4glX0WPZtO",
        "outputId": "d6a340dd-f1a5-4fe7-88aa-eacf09405995"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting vit_keras\n",
            "  Downloading vit_keras-0.1.2-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from vit_keras) (1.11.4)\n",
            "Collecting validators (from vit_keras)\n",
            "  Downloading validators-0.28.1-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->vit_keras) (1.25.2)\n",
            "Installing collected packages: validators, vit_keras\n",
            "Successfully installed validators-0.28.1 vit_keras-0.1.2\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (24.0)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow_addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "!pip install vit_keras\n",
        "!pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Afx-Gkx9Qwpu",
        "outputId": "b5141aa0-a113-4a58-caf3-f0b2c2564fbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# filename = '/content/drive/My Drive/C2D2 Datasets/dataset_3d_new.h5' # this file contains 836 stacks of images only of Lahore\n",
        "filename = '/content/drive/My Drive/C2D2 Datasets/dataset-001.h5' # this file contains 1996 stacks of images of Lahore, Aleppo, and Kathmandu (entire C2D2 dataset)\n",
        "\n",
        "import numpy as np\n",
        "import h5py\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "from vit_keras import vit\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ld4fkPf8WXqf",
        "outputId": "9100e76e-c413-47a5-ba68-bc5661cff7d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "140/140 [==============================] - 138s 613ms/step - loss: 1.0918 - accuracy: 0.5236 - precision: 0.5917 - recall: 0.3883 - val_loss: 0.8972 - val_accuracy: 0.6200 - val_precision: 0.6800 - val_recall: 0.5100\n",
            "Epoch 2/10\n",
            "140/140 [==============================] - 82s 589ms/step - loss: 0.7670 - accuracy: 0.6612 - precision: 0.7297 - recall: 0.5802 - val_loss: 0.7850 - val_accuracy: 0.6650 - val_precision: 0.7273 - val_recall: 0.5600\n",
            "Epoch 3/10\n",
            "140/140 [==============================] - 82s 589ms/step - loss: 0.5978 - accuracy: 0.7557 - precision: 0.8162 - recall: 0.6841 - val_loss: 0.8027 - val_accuracy: 0.6750 - val_precision: 0.7101 - val_recall: 0.6000\n",
            "Epoch 4/10\n",
            "140/140 [==============================] - 82s 589ms/step - loss: 0.5125 - accuracy: 0.8001 - precision: 0.8375 - recall: 0.7421 - val_loss: 0.8050 - val_accuracy: 0.6550 - val_precision: 0.6919 - val_recall: 0.5950\n",
            "Epoch 5/10\n",
            "140/140 [==============================] - 82s 589ms/step - loss: 0.3989 - accuracy: 0.8517 - precision: 0.8813 - recall: 0.8188 - val_loss: 0.7734 - val_accuracy: 0.7050 - val_precision: 0.7542 - val_recall: 0.6750\n",
            "Epoch 6/10\n",
            "140/140 [==============================] - 82s 589ms/step - loss: 0.3054 - accuracy: 0.9062 - precision: 0.9221 - recall: 0.8739 - val_loss: 0.7834 - val_accuracy: 0.7000 - val_precision: 0.7312 - val_recall: 0.6800\n",
            "Epoch 7/10\n",
            "140/140 [==============================] - 82s 589ms/step - loss: 0.2460 - accuracy: 0.9219 - precision: 0.9346 - recall: 0.9011 - val_loss: 0.8490 - val_accuracy: 0.6900 - val_precision: 0.7151 - val_recall: 0.6650\n",
            "Epoch 8/10\n",
            "140/140 [==============================] - 82s 589ms/step - loss: 0.1824 - accuracy: 0.9542 - precision: 0.9627 - recall: 0.9420 - val_loss: 0.8353 - val_accuracy: 0.6800 - val_precision: 0.7059 - val_recall: 0.6600\n",
            "Epoch 9/10\n",
            "140/140 [==============================] - 82s 589ms/step - loss: 0.1331 - accuracy: 0.9713 - precision: 0.9753 - recall: 0.9620 - val_loss: 0.8591 - val_accuracy: 0.7100 - val_precision: 0.7225 - val_recall: 0.6900\n",
            "Epoch 10/10\n",
            "140/140 [==============================] - 82s 588ms/step - loss: 0.0943 - accuracy: 0.9850 - precision: 0.9877 - recall: 0.9814 - val_loss: 0.8705 - val_accuracy: 0.7050 - val_precision: 0.7211 - val_recall: 0.6850\n"
          ]
        }
      ],
      "source": [
        "# Function to load and prepare the data\n",
        "def load_and_split_data(filepath, num_samples=None, test_size=0.2, val_size=0.1):\n",
        "    with h5py.File(filepath, 'r') as h5file:\n",
        "        data_x = h5file['data_x'][:]\n",
        "        data_y = h5file['data_y'][:]\n",
        "\n",
        "        # Normalization\n",
        "        data_x = data_x.astype('float32') / 255.0  \n",
        "\n",
        "        if num_samples is not None:\n",
        "            # Selecting subset of data if running over few samples\n",
        "            data_x = data_x[:num_samples]  \n",
        "            data_y = data_y[:num_samples]\n",
        "\n",
        "        # Splitting the original stacks into training, validation, and testing\n",
        "        train_x, test_x, train_y, test_y = train_test_split(\n",
        "            data_x, data_y, test_size=test_size, random_state=42\n",
        "        )\n",
        "        train_x, val_x, train_y, val_y = train_test_split(\n",
        "            train_x, train_y, test_size=val_size / (1 - test_size), random_state=42\n",
        "        )\n",
        "\n",
        "        # This function creates image pairs using only the first and third images \n",
        "        # (other versions of the file include making pairs of first and second, second and third, and first and third image)\n",
        "        def create_pairs(x, y):\n",
        "            \n",
        "            images_first_third = np.stack((x[:, 0, :, :, :], x[:, 2, :, :, :]), axis=1)\n",
        "\n",
        "            # Directly return pairs and labels\n",
        "            return images_first_third, y  \n",
        "\n",
        "        images_train, labels_train = create_pairs(train_x, train_y)\n",
        "        images_val, labels_val = create_pairs(val_x, val_y)\n",
        "        images_test, labels_test = create_pairs(test_x, test_y)\n",
        "\n",
        "        labels_train = to_categorical(labels_train, num_classes=4)\n",
        "        labels_val = to_categorical(labels_val, num_classes=4)\n",
        "        labels_test = to_categorical(labels_test, num_classes=4)\n",
        "\n",
        "        return images_train, images_val, images_test, labels_train, labels_val, labels_test\n",
        "\n",
        "def create_siamese_vit_network(input_shape):\n",
        "    base_network = create_vit_base_network(image_size=input_shape[1])\n",
        "\n",
        "    input_a = Input(shape=input_shape[1:])\n",
        "    input_b = Input(shape=input_shape[1:])\n",
        "\n",
        "    processed_a = base_network(input_a)\n",
        "    processed_b = base_network(input_b)\n",
        "\n",
        "    combined_features = concatenate([processed_a, processed_b], axis=-1)\n",
        "    combined_features = Dropout(0.1)(combined_features)\n",
        "    classification_layer = Dense(128, activation='relu')(combined_features)\n",
        "    outputs = Dense(4, activation='softmax')(classification_layer)\n",
        "\n",
        "    model = Model(inputs=[input_a, input_b], outputs=outputs)\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.00001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy', 'Precision', 'Recall']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Creating the Vision Transformer base network: vit_b16 trained on ImageNet\n",
        "def create_vit_base_network(image_size):\n",
        "    vit_model = vit.vit_b16(\n",
        "        image_size=image_size,\n",
        "        activation='softmax',\n",
        "        pretrained=True,\n",
        "        include_top=False,\n",
        "        pretrained_top=False\n",
        "    )\n",
        "    return vit_model\n",
        "\n",
        "images_train, images_val, images_test, labels_train, labels_val, labels_test = load_and_split_data(filename)\n",
        "\n",
        "model = create_siamese_vit_network(input_shape=(2, 256, 256, 3))\n",
        "\n",
        "# Use early stopping (note to self: remember to also uncomment last line of history block)\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    [images_train[:, 0], images_train[:, 1]], labels_train,\n",
        "    validation_data=([images_val[:, 0], images_val[:, 1]], labels_val),\n",
        "    epochs=10,\n",
        "    batch_size=10\n",
        "    # callbacks=[early_stopping]\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW7-k_7_n-gv",
        "outputId": "b42c3ab1-78ee-426f-8f24-c26c0a9640d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13/13 [==============================] - 14s 591ms/step - loss: 0.8183 - accuracy: 0.7225 - precision: 0.7330 - recall: 0.7000\n",
            "Test Loss: 0.8182976245880127\n",
            "Test Accuracy: 0.7225000262260437\n",
            "Test Precision: 0.7329843044281006\n",
            "Test Recall: 0.699999988079071\n"
          ]
        }
      ],
      "source": [
        "# Evaluating on test dataset: \n",
        "test_metrics = model.evaluate([images_test[:, 0], images_test[:, 1]], labels_test)\n",
        "print(f\"Test Loss: {test_metrics[0]}\")\n",
        "print(f\"Test Accuracy: {test_metrics[1]}\")\n",
        "print(f\"Test Precision: {test_metrics[2]}\")\n",
        "print(f\"Test Recall: {test_metrics[3]}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
